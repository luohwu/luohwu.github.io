
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/html">

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>HoloRegi</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://jonbarron.info/mipnerf360/"/>
    <meta property="og:title" content="Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields" />
    <meta property="og:description" content="Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on 'unbounded' scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub 'mip-NeRF 360' as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields" />
    <meta name="twitter:description" content="Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on 'unbounded' scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub 'mip-NeRF 360' as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes." />
    <meta name="twitter:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’«</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>HoloRegi: Patient Registration with HoloLens</b>
<!--                <b>Mip-NeRF 360</b>: Unbounded <br> Anti-Aliased Neural Radiance Fields</br> -->
<!--                <small>-->
<!--								CVPR 2022 (Oral Presentation)-->
<!--                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        Juyi Zhang <sup>*</sup>
                        </br>ETHZ
                    </li>
                    <li>
                        Junhao Zhang <sup>*</sup>
                        </br>ETHZ
                    </li>
                    <li>
                        <a href="https://luohwu.github.io">
                          Luohong Wu <sup>*</sup>
                        </a>
                        </br>ETHZ
                    </li>
                </ul>
                <br>
                <sup>*</sup> equal contribution

            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
<!--                        <li>-->
<!--                            <a href="https://luohwu.github.io/Faster SURF/paper/MAKING_SPEEDED_UP_ROBUST_FEATURES__SURF__FASTER.pdf">-->
<!--                                <image src="img/paper_image.jpg" height="60px"></image>>-->
<!--                                <h4><strong>Paper</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="https://youtu.be/zBSH-k9GbV4">-->
<!--                            <image src="img/youtube_icon.png" height="60px">-->
<!--                                <h4><strong>Video</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="http://storage.googleapis.com/gresearch/refraw360/360_v2.zip">-->
<!--                            <image src="img/database_icon.png" height="60px">-->
<!--                                <h4><strong>Dataset</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
                        <!-- <li>
                            <a href="TODO">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>



<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <video id="v0" width="100%" autoplay loop muted controls>-->
<!--                  <source src="img/gardenvase_720.mp4" type="video/mp4" />-->
<!--                </video>-->
<!--						</div>-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--							<p class="text-center">-->
<!--							Rendered images and depths from our model.-->
<!--							</p>-->
<!--						</div>-->
<!--        </div>-->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Background
                </h3>
                This was initially a team project of the course '3D Vision' at ETHZ, proposed by <a href="https://www.augmedit.com/">AugmedIT</a>.
                We were supervised by Jonas Hein (PhD student at ETHZ), Jene Meulstee (product developer of AugmedIT), and Tristan van Doormaal (one of founders of AugmedIT).
                After the course was finished, <a href="https://www.augmedit.com/">AugmedIT</a> extends this project into an internship (ongoing at this moment).
                <br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Introduction
                </h3>
                With augmented reality (AR), virtual 3D models can be superimposed into the real world of the userâ€™s view.
                <a href="https://www.augmedit.com/">AugmedIT</a> has created a workflow that can automatically transform MRI scans of patients with brain tumours <a href="https://pubmed.ncbi.nlm.nih.gov/33515122/">[1]</a>, into
                3D holograms for the HoloLens2. These holograms can be used by doctors to prepare or plan their surgical procedure.
                However, it remains challenging how these 3D holograms can be fused with the real
                patients. In the existing method, a pointer is used to indicate manually 5 points on both hologram and
                patient to register the hologram.

                <br><br>
                In this project, a semi-automatic process is proposed to register existing medical 3D holograms to patients. We first utilize the
                research mode of Hololens2 to obtain RGB-Depth data of a human head. The data is then transferred to a remote server via
                a TCP connection for processing. Next, we utilize a multiway-registration-based method to align the existing patient data,
                such as MRI data, with the processed point cloud. Visualizations and experiments demonstrate the success of our registration
                method. Our method outperforms the previous method by reducing the Fiducial Registration Error from 8.5 mm to 1.51 mm.
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview of our registration pipeline.
                </h3>

                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://youtube.com/embed/D5AF6lFXCTs" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Details
                </h3>
                <strong>Unfortunately, we can not release our documentations and code.</strong> If you are interested, please refer to:
                <br>
                <br>
                [1] T. Fick, J. van Doormaal, E. Hoving, L. Regli, T van Doormaal - Holographic patient tracking after bed movement for augmented reality neuronavigation using a head-mounted display. Acta Neurochir (Wien). 2021 Jan 29. doi: 10.1007/s00701-021-04707-4.
            </div>
        </div>


            


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                </p>
            </div>
        </div>

    </div>
</body>
</html>
